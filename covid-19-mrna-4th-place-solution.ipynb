{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth place solution to the \"OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction\"\n",
    "\n",
    "This notebook provides the code that can be used to generate our fourth place solution. Our final submission consisted of a blend of multiple variations of this notebook. The variations were only in the 2 RNN layers of the classification part of our network (LSTM+LSTM, LSTM+GRU, GRU+LSTM, GRU+GRU).\n",
    "\n",
    "This notebook is mainly based on [this great notebook](https://www.kaggle.com/mrkmakr/covid-ae-pretrain-gnn-attn-cnn) from [@mrmakr](https://www.kaggle.com/mrkmakr).\n",
    "\n",
    "Different datasets are used:\n",
    "* **Augmented data:** the output from [@its7171](https://www.kaggle.com/its7171) [his notebook](https://www.kaggle.com/its7171/how-to-generate-augmentation-data). This dataset contains different possible structures for the same RNA sequence.\n",
    "* **BPPs generated with ARNIE:** we generated BPPs with different packages using ARNIE. We [provide a notebook te demonstrate this](https://www.kaggle.com/group16/generating-bpps-with-arnie). We also re-use the structures to calculate a certainty feature (fraction of packages that agree on a certain predicted structure).\n",
    "* **Predicted loop types with CapR** [CapR](https://github.com/fukunagatsu/CapR) generates probabilities for each of the loop types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Processing the data\n",
    "\n",
    "We will create the following inputs for the train set, public test set and private test set:\n",
    "* A `NxLxLxE` adjacency matrix, with `N` the number of samples, `L` the sequence length and `E` the number of adjacency (edge) features.\n",
    "* A `NxLxV` node feature matrix, with `N` number of samples, `L` sequence length and `V` number of node features (features per base of the RNA sequence)\n",
    "* A `NxT` target matrix, with `T`=5 our 5 targets. We also train on the non-scored targets as that improved overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"../input/stanford-covid-vaccine/train.json\",lines=True)\n",
    "test  = pd.read_json(\"../input/stanford-covid-vaccine/test.json\",lines=True)\n",
    "sub = pd.read_csv(\"../input/stanford-covid-vaccine/sample_submission.csv\")\n",
    "\n",
    "test_pub = test[test[\"seq_length\"] == 107]\n",
    "test_pri = test[test[\"seq_length\"] == 130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Appending the augmentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = pd.read_csv('../input/covid19-mrna-augmentation-data-and-features/aug_data1.csv')\n",
    "aug_df = aug_df.drop_duplicates(subset=['id', 'structure'])\n",
    "\n",
    "\n",
    "def aug_data(df):\n",
    "    target_df = df.copy()\n",
    "    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n",
    "                         \n",
    "    del target_df['structure']\n",
    "    del target_df['predicted_loop_type']\n",
    "    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n",
    "\n",
    "    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n",
    "    df['log_gamma'] = 100\n",
    "    df['score'] = 1.0\n",
    "    df = df.append(new_df[df.columns])\n",
    "    return df\n",
    "\n",
    "train = aug_data(train).reset_index(drop=True)\n",
    "test_pub = aug_data(test_pub).reset_index(drop=True)\n",
    "test_pri = aug_data(test_pri).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Creating adjacency matrices\n",
    "\n",
    "We extract 5 edge features:\n",
    "* 3 different BPPs: the provided ones, generated with rnasoft (ARNIE) and generated with contrafold (ARNIE)\n",
    "* whether there is a base pairing indicated in the structure ( `(` and `)` )\n",
    "* the distances (manhattan) between bases, normalized by sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The provided BPPs\n",
    "As = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    a = np.load(f\"../input/stanford-covid-vaccine/bpps/{id}.npy\").astype(np.float16)\n",
    "    As.append(a)\n",
    "As = np.array(As)\n",
    "\n",
    "As_pub = []\n",
    "for id in tqdm(test_pub[\"id\"]):\n",
    "    a = np.load(f\"../input/stanford-covid-vaccine/bpps/{id}.npy\").astype(np.float16)\n",
    "    As_pub.append(a)\n",
    "As_pub = np.array(As_pub)\n",
    "\n",
    "As_pri = []\n",
    "for id in tqdm(test_pri[\"id\"]):\n",
    "    a = np.load(f\"../input/stanford-covid-vaccine/bpps/{id}.npy\").astype(np.float16)\n",
    "    As_pri.append(a)\n",
    "As_pri = np.array(As_pri)\n",
    "\n",
    "# BPPs generated with ARNIE (package='contrafold2')\n",
    "As_cf = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_contrafold_gamma1/bpps_contrafold_gamma1/bpps_contrafold_gamma1/{id}.npy\").astype(np.float16)\n",
    "    As_cf.append(a)\n",
    "As_cf = np.array(As_cf)\n",
    "\n",
    "As_pub_cf = []\n",
    "for id in tqdm(test_pub[\"id\"]):\n",
    "    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_contrafold_gamma1/bpps_contrafold_gamma1/bpps_contrafold_gamma1/{id}.npy\").astype(np.float16)\n",
    "    As_pub_cf.append(a)\n",
    "As_pub_cf = np.array(As_pub_cf)\n",
    "\n",
    "As_pri_cf = []\n",
    "for id in tqdm(test_pri[\"id\"]):\n",
    "    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_contrafold_gamma1/bpps_contrafold_gamma1/bpps_contrafold_gamma1/{id}.npy\").astype(np.float16)\n",
    "    As_pri_cf.append(a)\n",
    "As_pri_cf = np.array(As_pri_cf)\n",
    "\n",
    "# BPPs generated with ARNIE (package='rnasoft')\n",
    "As_rs = []\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/{id}.npy\").astype(np.float16)\n",
    "    As_rs.append(a)\n",
    "As_rs = np.array(As_rs)\n",
    "\n",
    "As_pub_rs = []\n",
    "for id in tqdm(test_pub[\"id\"]):\n",
    "    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/{id}.npy\").astype(np.float16)\n",
    "    As_pub_rs.append(a)\n",
    "As_pub_rs = np.array(As_pub_rs)\n",
    "\n",
    "As_pri_rs = []\n",
    "for id in tqdm(test_pri[\"id\"]):\n",
    "    a = np.load(f\"../input/covid19-mrna-augmentation-data-and-features/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/bpps_rnasoft_gamma1/{id}.npy\").astype(np.float16)\n",
    "    As_pri_rs.append(a)\n",
    "As_pri_rs = np.array(As_pri_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "def get_structure_adj(train):\n",
    "    Ss = []\n",
    "    for i in tqdm(range(len(train))):\n",
    "        seq_length = train[\"seq_length\"].iloc[i]\n",
    "        structure = train[\"structure\"].iloc[i]\n",
    "        sequence = train[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        a_structure = np.zeros([seq_length, seq_length])\n",
    "        for i in range(seq_length):\n",
    "            if structure[i] == \"(\":\n",
    "                cue.append(i)\n",
    "            elif structure[i] == \")\":\n",
    "                start = cue.pop()\n",
    "                a_structures[(sequence[start], sequence[i])][start, i] = 1\n",
    "                a_structures[(sequence[i], sequence[start])][i, start] = 1\n",
    "        \n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n",
    "        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n",
    "        Ss.append(a_strc)\n",
    "    \n",
    "    Ss = np.array(Ss)\n",
    "    return Ss.astype(np.float16)\n",
    "\n",
    "Ss = get_structure_adj(train)\n",
    "Ss_pub = get_structure_adj(test_pub)\n",
    "Ss_pri = get_structure_adj(test_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(As):\n",
    "    idx = np.arange(As.shape[1])\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = Ds / Ds.shape[1]\n",
    "    Ds = Ds[None, :,:]\n",
    "    Ds = np.repeat(Ds, len(As), axis = 0)\n",
    "    \n",
    "    Dss = []\n",
    "    for i in [1]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis = 3)\n",
    "    return Ds.astype(np.float16)\n",
    "\n",
    "Ds = get_distance_matrix(As)\n",
    "Ds_pub = get_distance_matrix(As_pub)\n",
    "Ds_pri = get_distance_matrix(As_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "As = np.concatenate([As[:,:,:,None], As_cf[:,:,:,None], As_rs[:,:,:,None], Ss, Ds], axis = 3).astype(np.float16)\n",
    "del Ss, Ds, As_cf, As_rs\n",
    "\n",
    "As_pub = np.concatenate([As_pub[:,:,:,None], As_pub_cf[:,:,:,None], As_pub_rs[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float16)\n",
    "del Ss_pub, Ds_pub, As_pub_cf, As_pub_rs\n",
    "\n",
    "As_pri = np.concatenate([As_pri[:,:,:,None], As_pri_cf[:,:,:,None], As_pri_rs[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float16)\n",
    "del Ss_pri, Ds_pri, As_pri_cf, As_pri_rs\n",
    "\n",
    "print(As.shape, As_pub.shape, As_pri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(As.shape[-1]):\n",
    "    ax[i].imshow(As[0, :, :, i].astype(np.float32))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Creating node features\n",
    "\n",
    "We extract 25 features per base in our sequence (107x25 features for train & public test, 130x25 features for private test):\n",
    "* One-hot-encoded base (AGCU) (4 features)\n",
    "* One-hot-encoded loop type (provided) (7 features)\n",
    "* one-hot-encoded positional feature (index % 3) (3 features)\n",
    "* certainty: fraction of packages (3 different ones) that predict `(`, `)` or `.`  (3 features)\n",
    "* CapR loop type probabilities (6 features)\n",
    "* BPP sum and number of zeroese (2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arnie_train_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/arnie/arnie/train_features.p', 'rb'))\n",
    "arnie_test_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/arnie/arnie/test_features.p', 'rb'))\n",
    "capr_train_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/CapR_train.pickle', 'rb'))\n",
    "capr_test_features = pickle.load(open('../input/covid19-mrna-augmentation-data-and-features/CapR_test.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequence\n",
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "def get_input(train):\n",
    "    mapping = {}\n",
    "    vocab = [\"A\", \"G\", \"C\", \"U\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    \n",
    "    X_pos1 = np.stack(train[\"sequence\"].apply(lambda x : np.arange(len(x)) % 3 == 0))[:,:,np.newaxis]\n",
    "    X_pos2 = np.stack(train[\"sequence\"].apply(lambda x : np.arange(len(x)) % 3 == 1))[:,:,np.newaxis]\n",
    "    X_pos3 = np.stack(train[\"sequence\"].apply(lambda x : np.arange(len(x)) % 3 == 2))[:,:,np.newaxis]\n",
    "\n",
    "    X_certainty = np.zeros((X_pos1.shape[:2] + (3, )))\n",
    "    X_loop_capr = np.zeros((X_pos1.shape[:2] + (6, )))\n",
    "    X_bpp_sum = np.zeros((X_pos1.shape[:2] + (1, )))\n",
    "    X_bpp_nb_zeroes = np.zeros((X_pos1.shape[:2] + (1, )))\n",
    "\n",
    "    for k, mol_id in enumerate(train['id'].values):\n",
    "        if mol_id in arnie_train_features:\n",
    "            arr1 = np.array(list(arnie_train_features[mol_id]['mfe']['vienna_2']))\n",
    "            arr2 = np.array(list(arnie_train_features[mol_id]['mfe']['contrafold_2']))\n",
    "            arr3 = np.array(list(arnie_train_features[mol_id]['mfe']['rnastructure']))\n",
    "        else:\n",
    "            arr1 = np.array(list(arnie_test_features[mol_id]['mfe']['vienna_2']))\n",
    "            arr2 = np.array(list(arnie_test_features[mol_id]['mfe']['contrafold_2']))\n",
    "            arr3 = np.array(list(arnie_test_features[mol_id]['mfe']['rnastructure']))\n",
    "\n",
    "        X_certainty[k, :, 0] = ((arr1 == '(') + (arr2 == '(') + (arr3 == '(')).astype(int) / 3\n",
    "        X_certainty[k, :, 1] = ((arr1 == ')') + (arr2 == ')') + (arr3 == ')')).astype(int) / 3\n",
    "        X_certainty[k, :, 2] = ((arr1 == '.') + (arr2 == '.') + (arr3 == '.')).astype(int) / 3\n",
    "        \n",
    "        if mol_id in capr_train_features:\n",
    "            X_loop_capr[k] = np.array([list(map(float, i.split()[1:])) for i in capr_train_features[mol_id].split('\\n')[1:-2]]).T\n",
    "        else:\n",
    "            X_loop_capr[k] = np.array([list(map(float, i.split()[1:])) for i in capr_test_features[mol_id].split('\\n')[1:-2]]).T\n",
    "\n",
    "        bpp = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").astype(np.float16)\n",
    "        X_bpp_sum[k] = bpp.sum(axis=1)[:, None]\n",
    "        X_bpp_nb_zeroes[k] = ((bpp == 0).sum(axis=1) / X_pos1.shape[1])[:, None]\n",
    "\n",
    "\n",
    "    X_node = np.concatenate([X_node, X_loop, X_pos1, X_pos2, X_pos3, X_certainty, X_loop_capr, X_bpp_sum, X_bpp_nb_zeroes], axis = 2)\n",
    "    return X_node\n",
    "\n",
    "X_node = get_input(train).astype(np.float16)\n",
    "X_node_pub = get_input(test_pub).astype(np.float16)\n",
    "X_node_pri = get_input(test_pri).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_node.shape, X_node_pub.shape, X_node_pri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del arnie_train_features, arnie_test_features, capr_train_features, capr_test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Extracting the targets\n",
    "\n",
    "5 targets, including the 2 non-scored ones (deg_pH10 and deg_50C) as they increase the performance of the 3 others when trained jointly on all 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
    "\n",
    "y_train = []\n",
    "seq_len = train[\"seq_length\"].iloc[0]\n",
    "seq_len_target = train[\"seq_scored\"].iloc[0]\n",
    "ignore = -10000\n",
    "ignore_length = seq_len - seq_len_target\n",
    "for target in targets:\n",
    "    y = np.vstack(train[target])\n",
    "    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n",
    "    y = np.hstack([y, dummy])\n",
    "    y_train.append(y)\n",
    "y = np.stack(y_train, axis = 2)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NODE_FEATURES = X_node.shape[2]\n",
    "N_EDGE_FEATURES = As.shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating the model & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_epochs = 25\n",
    "ae_epochs_each = 5\n",
    "ae_batch_size = 32\n",
    "\n",
    "epochs_list = [30, 10, 3, 3, 5, 5]\n",
    "batch_size_list = [8, 16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcrmse(t, p, seq_len_target = seq_len_target):\n",
    "    score = np.mean(np.sqrt(tf.keras.losses.mean_squared_error(t, p))[:, :seq_len_target])\n",
    "    return score\n",
    "\n",
    "def rmse(y_actual, y_pred):\n",
    "    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n",
    "    return K.sqrt(mse)\n",
    "\n",
    "def mcrmse_loss(y_actual, y_pred, num_scored=5):\n",
    "    y_actual = y_actual[:, :68, :]\n",
    "    y_pred = y_pred[:, :68, :]\n",
    "    score = 0\n",
    "    for i in range(num_scored):\n",
    "        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) / num_scored\n",
    "    return score\n",
    "\n",
    "def attention(x_inner, x_outer, n_factor, dropout):\n",
    "    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_inner)\n",
    "    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(n_factor, \n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform',\n",
    "                     )(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def res(x, unit, kernel = 3, rate = 0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "def forward(x, unit, kernel = 3, rate = 0.1):\n",
    "#     h = L.Dense(unit, None)(x)\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "#         h = tf.keras.activations.swish(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for i in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = tf.matmul(adj, x_a)\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(config):\n",
    "    node = tf.keras.Input(shape = (None, N_NODE_FEATURES), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, N_EDGE_FEATURES), name = \"adj\")\n",
    "    \n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n",
    "        \n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n",
    "    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n",
    "    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n",
    "    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "    \n",
    "    for unit in [128, 64]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel = 30)\n",
    "        \n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit)\n",
    "        x = multi_head_attention(x, x, unit, 4, 0.0)\n",
    "        xs.append(x)\n",
    "        \n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base, config):\n",
    "    node = tf.keras.Input(shape = (None, N_NODE_FEATURES), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, N_EDGE_FEATURES), name = \"adj\")\n",
    "\n",
    "    x = base([L.SpatialDropout1D(0.3)(node), adj])\n",
    "    x = forward(x, 64, rate = 0.3)\n",
    "    p = L.Dense(N_NODE_FEATURES, \"sigmoid\")(x)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.math.abs(node - p))\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = lambda t, y : y)\n",
    "    return model\n",
    "\n",
    "def gru_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n",
    "def lstm_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n",
    "\n",
    "def get_model(base, config):\n",
    "    node = tf.keras.Input(shape = (None, N_NODE_FEATURES), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, N_EDGE_FEATURES), name = \"adj\")\n",
    "    \n",
    "    x = base([node, adj])\n",
    "    x = lstm_layer(256, 0.5)(x)\n",
    "    x = lstm_layer(256, 0.5)(x)\n",
    "    x = L.Dense(5, None)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = mcrmse_loss)\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "    adam = tf.optimizers.Adam()\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "if ae_epochs > 0:\n",
    "    base = get_base(config)\n",
    "    ae_model = get_ae_model(base, config)\n",
    "    ## TODO : simultaneous train\n",
    "    for i in range(ae_epochs//ae_epochs_each):\n",
    "        print(f\"------ {i} ------\")\n",
    "        print(\"--- train ---\")\n",
    "        X_node_shuff, As_shuff = shuffle(X_node, As)\n",
    "        ae_model.fit([X_node_shuff, As_shuff], [np.zeros((len(X_node)))],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        \n",
    "        print(\"--- public ---\")\n",
    "        X_node_pub_shuff, As_pub_shuff = shuffle(X_node_pub, As_pub)\n",
    "        ae_model.fit([X_node_pub_shuff, As_pub_shuff], [np.zeros((len(X_node_pub)))],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        \n",
    "        print(\"--- private ---\")\n",
    "        X_node_pri_shuff, As_pri_shuff = shuffle(X_node_pri, As_pri)\n",
    "        ae_model.fit([X_node_pri_shuff, As_pri_shuff], [np.zeros((len(X_node_pri)))],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        gc.collect()\n",
    "    print(\"****** save ae model ******\")\n",
    "    base.save_weights(\"base_ae_lstm_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ae_model\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_node_pub.npy', X_node_pub)\n",
    "np.save('X_node_pri.npy', X_node_pri)\n",
    "np.save('As_pub.npy', As_pub)\n",
    "np.save('As_pri.npy', As_pri)\n",
    "\n",
    "del X_node_pub, X_node_pri, As_pub, As_pri, X_node_shuff, As_shuff, X_node_pub_shuff, As_pub_shuff, X_node_pri_shuff, As_pri_shuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = GroupKFold(5)\n",
    "\n",
    "config = {}\n",
    "scores = []\n",
    "preds = np.zeros([len(X_node), X_node.shape[1], 5])\n",
    "X_node, As, y, weights, groups_shuffled, SN_filter_mask = shuffle(X_node, As, y, train.signal_to_noise.values, train['id'], (train['SN_filter'] == 1).values)\n",
    "del train\n",
    "for i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As, groups_shuffled)):\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    X_node_tr = X_node[tr_idx]\n",
    "    X_node_va = X_node[va_idx]\n",
    "    As_tr = As[tr_idx]\n",
    "    As_va = As[va_idx]  \n",
    "    y_tr = y[tr_idx]\n",
    "    y_va = y[va_idx]\n",
    "    w_trn = np.log(weights[tr_idx]+1.105)/2\n",
    "    \n",
    "    base = get_base(config)\n",
    "    if ae_epochs > 0:\n",
    "        print(\"****** load ae model ******\")\n",
    "        base.load_weights(\"base_ae_lstm_lstm\")\n",
    "    model = get_model(base, config)\n",
    "    for epochs, batch_size in zip(epochs_list, batch_size_list):\n",
    "        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n",
    "        model.fit([X_node_tr, As_tr], [y_tr],\n",
    "                  validation_data=([X_node_va[SN_filter_mask[va_idx]], As_va[SN_filter_mask[va_idx]]], [y_va[SN_filter_mask[va_idx]]]),\n",
    "                  epochs = epochs,\n",
    "                  sample_weight=w_trn/2,\n",
    "                  batch_size = batch_size, validation_freq = 3)\n",
    "        \n",
    "    model.save_weights(f\"model{i}_lstm_lstm\")\n",
    "    p = model.predict([X_node_va, As_va])\n",
    "    for col in range(5):\n",
    "        print(targets[col], ((p[:, :68, col][SN_filter_mask[va_idx]] - y_va[:,:68,col][SN_filter_mask[va_idx]]) ** 2).mean() ** .5)\n",
    "    scores.append(np.mean(mcrmse_loss(y_va, p)))\n",
    "    \n",
    "    print(f\"fold {i}: mcrmse {scores[-1]}\")\n",
    "    preds[va_idx] = p\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = []\n",
    "for p_ix, _id in zip(range(preds.shape[0]), groups_shuffled):\n",
    "    for i in range(68):\n",
    "        preds_df.append([f'{_id}_{i}', preds[p_ix, i, 0], preds[p_ix, i, 1], \n",
    "                         preds[p_ix, i, 3], y[p_ix, i, 0], y[p_ix, i, 1],\n",
    "                         y[p_ix, i, 3], SN_filter_mask[p_ix]])\n",
    "preds_df = pd.DataFrame(preds_df, columns=['id_seqpos', 'reactivity', 'deg_Mg_pH10', \n",
    "                                           'deg_Mg_50C', 'reactivity_gt', 'deg_Mg_pH10_gt', \n",
    "                                           'deg_Mg_50C_gt', 'SN_filter'])\n",
    "preds_df = preds_df.groupby('id_seqpos').mean()\n",
    "rmses = []\n",
    "mses = []\n",
    "for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n",
    "    _rmse = ((preds_df[preds_df['SN_filter'] == 1][col] - preds_df[preds_df['SN_filter'] == 1][col+'_gt']) ** 2).mean() ** .5\n",
    "    _mse = ((preds_df[preds_df['SN_filter'] == 1][col] - preds_df[preds_df['SN_filter'] == 1][col+'_gt']) ** 2).mean()\n",
    "    rmses.append(_rmse)\n",
    "    mses.append(_mse)\n",
    "    print(col, _rmse, _mse)\n",
    "print(np.mean(rmses), np.mean(mses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_node, As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_node_pub = np.load('X_node_pub.npy')\n",
    "X_node_pri = np.load('X_node_pri.npy')\n",
    "As_pub = np.load('As_pub.npy')\n",
    "As_pri = np.load('As_pri.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pub = 0\n",
    "p_pri = 0\n",
    "for i in range(5):\n",
    "    config = {}\n",
    "    base = get_base(config)\n",
    "    if ae_epochs > 0:\n",
    "        print(\"****** load ae model ******\")\n",
    "        base.load_weights(\"base_ae_lstm_lstm\")\n",
    "    model = get_model(base, config)\n",
    "    model.load_weights(f\"model{i}_lstm_lstm\")\n",
    "    p_pub += model.predict([X_node_pub, As_pub]) / 5\n",
    "    p_pri += model.predict([X_node_pri, As_pri]) / 5\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n",
    "    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "for df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=targets)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)\n",
    "preds_df = preds_df.groupby('id_seqpos').mean()\n",
    "preds_df.to_csv(\"submission_lstm_lstm.csv\")\n",
    "preds_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
